import json
import gradio as gr
from langchain.document_loaders import DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OllamaEmbeddings
from langchain.vectorstores import Chroma
from langchain.llms import Ollama
from langchain.chains import RetrievalQA

# ======== CONFIGURA√á√ïES ==========
with open("config.json", "r", encoding="utf-8") as f:
    cfg = json.load(f)

NOME = cfg["nome"]
EMPRESA = cfg["empresa"]
DESCRICAO = cfg["descricao"]
TOM = cfg["tom"]
MODELO = cfg["modelo"]

# ======== CARREGAR OS DADOS ========
print(f"üìÇ Carregando dados para {EMPRESA}...")
loader = DirectoryLoader("dados", show_progress=True)
docs = loader.load()

splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
texts = splitter.split_documents(docs)

# ======== CRIAR EMBEDDINGS E BASE VETORIAL ========
print("‚öôÔ∏è Criando base vetorial...")
embeddings = OllamaEmbeddings(model=MODELO)
db = Chroma.from_documents(texts, embeddings)

# ======== CONFIGURAR O MODELO LOCAL ========
llm = Ollama(model=MODELO)

# ======== CONFIGURA√á√ÉO DE PERSONALIDADE ========
system_prompt = f"""
Voc√™ √© {NOME}, atendente da empresa {EMPRESA}.
Seu papel √© realizar o atendimento humano de clientes e funcion√°rios, fornecendo informa√ß√µes sobre produtos, servi√ßos e aplica√ß√µes de ve√≠culos.
Voc√™ deve se expressar como um atendente real ‚Äî natural, educado e prestativo ‚Äî sem mencionar que √© um sistema, rob√¥ ou intelig√™ncia artificial.

Comportamento esperado:
- Use linguagem humana e espont√¢nea (ex: "Claro, posso te ajudar com isso" ou "Deixa eu confirmar aqui pra voc√™").
- Sempre que poss√≠vel, informe o nome do produto, refer√™ncia e aplica√ß√£o no ve√≠culo.
- Se n√£o tiver certeza de uma resposta, diga algo como "vou confirmar essa informa√ß√£o com o setor t√©cnico e te retorno".
- Nunca diga frases como "sou uma IA", "fui treinado", "modelo de linguagem", etc.
- Adapte-se ao nome da empresa informado no arquivo de configura√ß√£o.
"""

# ======== CHAT RAG ========
qa = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=db.as_retriever(),
    chain_type="stuff",
    chain_type_kwargs={"verbose": False, "prompt": None}
)

# ======== INTERFACE (Gradio) ========
def responder(pergunta):
    prompt = f"{system_prompt}\n\nCliente: {pergunta}\n{NOME}:"
    resposta = qa.run(prompt)
    return resposta.strip()

iface = gr.Interface(
    fn=responder,
    inputs=gr.Textbox(label="Pergunta do cliente"),
    outputs="text",
    title=f"{EMPRESA} ‚Äî Atendimento ao Cliente",
    description=f"Converse com {NOME}, atendente da {EMPRESA}."
)

print(f"üöÄ {NOME} iniciada para {EMPRESA}! Acesse o link abaixo:")
iface.launch()
